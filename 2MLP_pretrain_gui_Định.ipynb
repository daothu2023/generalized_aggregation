{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daothu2023/generalized_aggregation/blob/main/2MLP_pretrain_gui_%C4%90%E1%BB%8Bnh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdH7m_BBTLmH",
        "outputId": "cb269455-a4f1-4495-8c5e-02958f9aecd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysGUjqPzTSpa",
        "outputId": "668f0a0f-a839-4f1c-d564-a8f1e785f369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# ---------- 1. ƒê·ªçc d·ªØ li·ªáu ----------\n",
        "edges_df = pd.read_csv('/content/drive/MyDrive/PPI_STRING/ppi_for_gnn_filled.csv')\n",
        "features_df = pd.read_csv('/content/drive/My Drive/PPI_STRING/features_for_BRCA.csv', index_col=0)\n",
        "label1_df = pd.read_csv('/content/drive/MyDrive/PPI_STRING/New_BRCA_labels(0_1).csv')\n",
        "label2_df = pd.read_csv('/content/drive/MyDrive/PPI_STRING/New_dataset_dinhgui/label_telomere.csv')\n",
        "\n",
        "# ---------- 2. Danh s√°ch t·∫•t c·∫£ gene t·ª´ PPI ----------\n",
        "genes_from_edges = set(edges_df['protein1']).union(set(edges_df['protein2']))\n",
        "genes_from_features = set(features_df.index)\n",
        "all_genes = sorted(genes_from_edges)  # ƒë·∫£m b·∫£o th·ª© t·ª± c·ªë ƒë·ªãnh\n",
        "\n",
        "# Mapping gene <-> index\n",
        "node_to_idx = {gene: i for i, gene in enumerate(all_genes)}\n",
        "idx_to_node = {i: gene for gene, i in node_to_idx.items()}\n",
        "\n",
        "# ---------- 3. edge_index ----------\n",
        "edges = edges_df[['protein1', 'protein2']].dropna()\n",
        "edge_index = torch.tensor([[node_to_idx[a], node_to_idx[b]]\n",
        "                           for a, b in edges.values if a in node_to_idx and b in node_to_idx],\n",
        "                          dtype=torch.long).t().contiguous()\n",
        "\n",
        "# ---------- 4. T·∫°o ƒë·∫∑c tr∆∞ng x ----------\n",
        "feature_dim = features_df.shape[1]\n",
        "x_matrix = np.zeros((len(all_genes), feature_dim))\n",
        "has_feature = np.zeros(len(all_genes), dtype=bool)\n",
        "\n",
        "# Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features_df.values)\n",
        "features_scaled_df = pd.DataFrame(features_scaled, index=features_df.index)\n",
        "\n",
        "# G√°n features cho nh·ªØng gene c√≥ s·∫µn\n",
        "for gene in features_scaled_df.index:\n",
        "    if gene in node_to_idx:\n",
        "        idx = node_to_idx[gene]\n",
        "        x_matrix[idx] = features_scaled_df.loc[gene].values\n",
        "        has_feature[idx] = True\n",
        "\n",
        "# T√≠nh h√†ng x√≥m\n",
        "neighbors_dict = {i: [] for i in range(len(all_genes))}\n",
        "for src, dst in edge_index.t().tolist():\n",
        "    neighbors_dict[src].append(dst)\n",
        "    neighbors_dict[dst].append(src)\n",
        "\n",
        "# G√°n ƒë·∫∑c tr∆∞ng trung b√¨nh h√†ng x√≥m cho node thi·∫øu\n",
        "for i in range(len(all_genes)):\n",
        "    if not has_feature[i]:\n",
        "        neighbor_feats = [x_matrix[n] for n in neighbors_dict[i] if has_feature[n]]\n",
        "        if neighbor_feats:\n",
        "            x_matrix[i] = np.mean(neighbor_feats, axis=0)\n",
        "        # n·∫øu kh√¥ng c√≥ h√†ng x√≥m n√†o c√≥ feature th√¨ gi·ªØ nguy√™n (to√†n 0)\n",
        "\n",
        "x = torch.tensor(x_matrix, dtype=torch.float)\n",
        "\n",
        "# -------- 5. T·∫°o nh√£n --------\n",
        "num_nodes = len(all_genes)\n",
        "y1 = torch.full((num_nodes,), -1, dtype=torch.long)\n",
        "y2 = torch.full((num_nodes,), -1, dtype=torch.long)\n",
        "\n",
        "label1_map = dict(zip(label1_df['Gene'], label1_df['Labels']))\n",
        "label2_map = dict(zip(label2_df['Gene'], label2_df['Labels']))\n",
        "\n",
        "for gene, label in label1_map.items():\n",
        "    if gene in node_to_idx:\n",
        "        y1[node_to_idx[gene]] = int(label)\n",
        "\n",
        "for gene, label in label2_map.items():\n",
        "    if gene in node_to_idx:\n",
        "        y2[node_to_idx[gene]] = int(label)\n",
        "\n",
        "# -------- 6. T·∫°o ƒë·ªëi t∆∞·ª£ng Data --------\n",
        "data = Data(x=x, edge_index=edge_index, y1=y1, y2=y2)\n",
        "\n",
        "# -------- 7. In th√¥ng tin --------\n",
        "print(data)\n",
        "print(f\"T·ªïng s·ªë ƒë·ªânh: {data.num_nodes}\")\n",
        "print(f\"T·ªïng s·ªë c·∫°nh: {data.num_edges}\")\n",
        "print(f\"S·ªë ƒë·ªânh c√≥ label1: {(y1 != -1).sum().item()}\")\n",
        "print(f\"S·ªë ƒë·ªânh c√≥ label2: {(y2 != -1).sum().item()}\")\n",
        "\n",
        "# -------- 8. Th·ªëng k√™ s·ªë l∆∞·ª£ng nh√£n 0 v√† 1 --------\n",
        "label1_0 = (y1 == 0).sum().item()\n",
        "label1_1 = (y1 == 1).sum().item()\n",
        "label2_0 = (y2 == 0).sum().item()\n",
        "label2_1 = (y2 == 1).sum().item()\n",
        "\n",
        "print(f\"üìä Label1 - S·ªë nh√£n 0: {label1_0}, nh√£n 1: {label1_1}\")\n",
        "print(f\"üìä Label2 - S·ªë nh√£n 0: {label2_0}, nh√£n 1: {label2_1}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Isw4JEWbMwrx",
        "outputId": "3b934285-0144-4e04-d8fb-b4185d54808c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[12809, 11], edge_index=[2, 243840], y1=[12809], y2=[12809])\n",
            "T·ªïng s·ªë ƒë·ªânh: 12809\n",
            "T·ªïng s·ªë c·∫°nh: 243840\n",
            "S·ªë ƒë·ªânh c√≥ label1: 1337\n",
            "S·ªë ƒë·ªânh c√≥ label2: 3286\n",
            "üìä Label1 - S·ªë nh√£n 0: 1290, nh√£n 1: 47\n",
            "üìä Label2 - S·ªë nh√£n 0: 1539, nh√£n 1: 1747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# ===== ƒê·ªãnh nghƒ©a GCNEncoder =====\n",
        "class GCNEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels):\n",
        "        super(GCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.fc1 = nn.Linear(hidden_channels, hidden_channels)\n",
        "        self.out_proj = nn.Linear(hidden_channels, in_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.out_proj(x)\n",
        "\n",
        "# ===== H√†m pretrain encoder v√† l∆∞u t·∫°i c√°c epoch mong mu·ªën =====\n",
        "def pretrain_encoder_save_multiple(data, hidden_channels=32, num_epochs=200,\n",
        "                                   mask_ratio=0.15, lr=0.01,\n",
        "                                   save_epochs=[50, 100, 150, 200],\n",
        "                                   save_folder=\"/content/drive/MyDrive/Pretrained_Encoders\"):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    data = data.to(device)\n",
        "\n",
        "    encoder = GCNEncoder(data.num_node_features, hidden_channels).to(device)\n",
        "    optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        encoder.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_masked = data.x.clone()\n",
        "        mask = torch.rand_like(x_masked) < mask_ratio\n",
        "        x_masked[mask] = 0\n",
        "\n",
        "        out = encoder(x_masked, data.edge_index)\n",
        "        loss = F.mse_loss(out, data.x)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:03d} | Pretrain Loss: {loss.item():.6f}\")\n",
        "\n",
        "        # === Save n·∫øu t·ªõi c√°c epoch mong mu·ªën\n",
        "        if epoch in save_epochs:\n",
        "            save_path = f\"{save_folder}/BRCA_encoder_epoch{epoch}.pth\"\n",
        "            torch.save(encoder.state_dict(), save_path)\n",
        "            print(f\"üíæ ƒê√£ l∆∞u encoder t·∫°i epoch {epoch} -> {save_path}\")\n",
        "\n",
        "    return encoder\n"
      ],
      "metadata": {
        "id": "r3z_YUG-LNuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrain v√† t·ª± ƒë·ªông l∆∞u t·∫°i 50, 100, 150, 200\n",
        "pretrain_encoder_save_multiple(\n",
        "    data,\n",
        "    hidden_channels=32,\n",
        "    num_epochs=200,\n",
        "    mask_ratio=0.15,\n",
        "    lr=0.01,\n",
        "    save_epochs=[50, 100, 150, 200],\n",
        "    save_folder=\"/content/drive/MyDrive/Pretrained_Encoders\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llGVQR1-NHGC",
        "outputId": "ec67d1fc-78fa-42e4-f747-b0a50c92f1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Pretrain Loss: 1.155570\n",
            "Epoch 010 | Pretrain Loss: 0.919709\n",
            "Epoch 020 | Pretrain Loss: 0.700116\n",
            "Epoch 030 | Pretrain Loss: 0.617062\n",
            "Epoch 040 | Pretrain Loss: 0.564116\n",
            "Epoch 050 | Pretrain Loss: 0.542802\n",
            "üíæ ƒê√£ l∆∞u encoder t·∫°i epoch 50 -> /content/drive/MyDrive/Pretrained_Encoders/BRCA_encoder_epoch50.pth\n",
            "Epoch 060 | Pretrain Loss: 0.524768\n",
            "Epoch 070 | Pretrain Loss: 0.519284\n",
            "Epoch 080 | Pretrain Loss: 0.513322\n",
            "Epoch 090 | Pretrain Loss: 0.507946\n",
            "Epoch 100 | Pretrain Loss: 0.491350\n",
            "üíæ ƒê√£ l∆∞u encoder t·∫°i epoch 100 -> /content/drive/MyDrive/Pretrained_Encoders/BRCA_encoder_epoch100.pth\n",
            "Epoch 110 | Pretrain Loss: 0.491382\n",
            "Epoch 120 | Pretrain Loss: 0.482907\n",
            "Epoch 130 | Pretrain Loss: 0.477237\n",
            "Epoch 140 | Pretrain Loss: 0.476862\n",
            "Epoch 150 | Pretrain Loss: 0.480598\n",
            "üíæ ƒê√£ l∆∞u encoder t·∫°i epoch 150 -> /content/drive/MyDrive/Pretrained_Encoders/BRCA_encoder_epoch150.pth\n",
            "Epoch 160 | Pretrain Loss: 0.477450\n",
            "Epoch 170 | Pretrain Loss: 0.469857\n",
            "Epoch 180 | Pretrain Loss: 0.465825\n",
            "Epoch 190 | Pretrain Loss: 0.469886\n",
            "Epoch 200 | Pretrain Loss: 0.461379\n",
            "üíæ ƒê√£ l∆∞u encoder t·∫°i epoch 200 -> /content/drive/MyDrive/Pretrained_Encoders/BRCA_encoder_epoch200.pth\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCNEncoder(\n",
              "  (conv1): GCNConv(11, 32)\n",
              "  (conv2): GCNConv(32, 32)\n",
              "  (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (out_proj): Linear(in_features=32, out_features=11, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import average_precision_score\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "ALPHA = 0.7\n",
        "\n",
        "class GCN_MultiHead(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels1, out_channels2):\n",
        "        super(GCN_MultiHead, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.mlp1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, hidden_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_channels, out_channels1)\n",
        "        )\n",
        "        self.mlp2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels, hidden_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_channels, out_channels2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        return self.mlp1(x), self.mlp2(x)\n",
        "\n",
        "def train_one_epoch_dual(model, data, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out1, out2 = model(data.x, data.edge_index)\n",
        "\n",
        "    mask1 = data.train_mask & (data.y1 != -1)\n",
        "    mask2 = data.y2 != -1  # full y2\n",
        "\n",
        "    loss1 = loss_fn(out1[mask1], data.y1[mask1]) if mask1.sum() > 0 else 0\n",
        "    loss2 = loss_fn(out2[mask2], data.y2[mask2]) if mask2.sum() > 0 else 0\n",
        "    total_loss = ALPHA * loss1 + (1 - ALPHA) * loss2\n",
        "\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss1.item(), loss2.item(), total_loss.item()\n",
        "\n",
        "def evaluate_y1_only(model, data, mask, loss_fn):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out1, _ = model(data.x, data.edge_index)\n",
        "        mask1 = mask & (data.y1 != -1)\n",
        "        loss1 = loss_fn(out1[mask1], data.y1[mask1]) if mask1.sum() > 0 else 0\n",
        "        acc1 = (out1[mask1].argmax(dim=1) == data.y1[mask1]).float().mean().item() if mask1.sum() > 0 else 0\n",
        "        auprc1 = 0\n",
        "        if (data.y1[mask1] == 1).sum() > 0:\n",
        "            probs1 = F.softmax(out1[mask1], dim=1)\n",
        "            auprc1 = average_precision_score(data.y1[mask1].cpu(), probs1[:, 1].cpu())\n",
        "    return acc1, auprc1, loss1.item()\n",
        "\n",
        "def oversample_features(x, y, idx):\n",
        "    labels = y[idx].cpu().numpy()\n",
        "    class_counts = Counter(labels)\n",
        "    max_class = max(class_counts.values())\n",
        "    new_x, new_y = [], []\n",
        "    for c in class_counts:\n",
        "        c_idx = idx[(y[idx] == c)]\n",
        "        repeats = max_class - class_counts[c]\n",
        "        if repeats > 0:\n",
        "            repeat_idx = c_idx.repeat((repeats // len(c_idx)) + 1)[:repeats]\n",
        "            new_x.append(x[repeat_idx])\n",
        "            new_y.append(y[repeat_idx])\n",
        "    if new_x:\n",
        "        x_added = torch.cat(new_x)\n",
        "        y_added = torch.cat(new_y)\n",
        "        return torch.cat([x, x_added]), torch.cat([y, y_added])\n",
        "    return x, y\n",
        "\n",
        "def run_kfold_training_with_pretrained_encoder(data, pretrained_encoder_path, hidden_channels=32):\n",
        "    # === 1. Load pretrained encoder ===\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    encoder = GCNEncoder(data.num_node_features, hidden_channels).to(device)\n",
        "    encoder.load_state_dict(torch.load(pretrained_encoder_path, map_location=device))\n",
        "    print(f\"‚úÖ Loaded pretrained encoder t·ª´: {pretrained_encoder_path}\")\n",
        "\n",
        "    labeled_idx = torch.where((data.y1 != -1) & (data.y2 != -1))[0]\n",
        "    labels_for_split = data.y1[labeled_idx]\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    results = []\n",
        "\n",
        "    for fold, (train_val_idx, test_idx) in enumerate(skf.split(labeled_idx, labels_for_split)):\n",
        "        print(f\"\\nüìÅ Fold {fold+1}/5\")\n",
        "\n",
        "        sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "        train_idx, val_idx = next(sss_val.split(train_val_idx, labels_for_split[train_val_idx]))\n",
        "\n",
        "        train_nodes = labeled_idx[train_val_idx][train_idx]\n",
        "        val_nodes = labeled_idx[train_val_idx][val_idx]\n",
        "        test_nodes = labeled_idx[test_idx]\n",
        "\n",
        "        # === 2. T·∫°o model t·ª´ encoder pretrain\n",
        "        model = GCN_MultiHead(data.num_node_features, hidden_channels, out_channels1=2, out_channels2=2).to(device)\n",
        "        model.conv1.load_state_dict(encoder.conv1.state_dict())\n",
        "        model.conv2.load_state_dict(encoder.conv2.state_dict())\n",
        "\n",
        "        # === 3. Chu·∫©n b·ªã d·ªØ li·ªáu\n",
        "        x_train = data.x[train_nodes]\n",
        "        y1_train = data.y1[train_nodes]\n",
        "        x_resampled, y1_resampled = oversample_features(x_train, y1_train, torch.arange(len(train_nodes)))\n",
        "\n",
        "        x_new = torch.cat([data.x, x_resampled[len(train_nodes):]], dim=0)\n",
        "        y1_new = torch.cat([data.y1, y1_resampled[len(train_nodes):]], dim=0)\n",
        "        y2_new = torch.cat([data.y2, torch.full((len(x_resampled) - len(train_nodes),), -1, dtype=torch.long)], dim=0)\n",
        "\n",
        "        n_total = x_new.size(0)\n",
        "        train_mask = torch.zeros(n_total, dtype=torch.bool)\n",
        "        val_mask = torch.zeros(n_total, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(n_total, dtype=torch.bool)\n",
        "\n",
        "        train_mask[train_nodes] = True\n",
        "        train_mask[len(train_nodes):] = True\n",
        "        val_mask[val_nodes] = True\n",
        "        test_mask[test_nodes] = True\n",
        "\n",
        "        data_new = Data(\n",
        "            x=x_new.to(device),\n",
        "            edge_index=data.edge_index.to(device),\n",
        "            y1=y1_new.to(device),\n",
        "            y2=y2_new.to(device),\n",
        "            train_mask=train_mask.to(device),\n",
        "            val_mask=val_mask.to(device),\n",
        "            test_mask=test_mask.to(device)\n",
        "        )\n",
        "\n",
        "        # === 4. Loss + Optimizer\n",
        "        classes = np.unique(y1_resampled.cpu().numpy())\n",
        "        weights = compute_class_weight(class_weight='balanced', classes=classes, y=y1_resampled.cpu().numpy())\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float32).to(device))\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        # === 5. Fine-tune\n",
        "        for epoch in range(1, 201):\n",
        "            loss1, loss2, train_loss = train_one_epoch_dual(model, data_new, optimizer, loss_fn)\n",
        "            acc1, auprc1, val_loss1 = evaluate_y1_only(model, data_new, data_new.val_mask, loss_fn)\n",
        "\n",
        "            print(f\"Epoch {epoch:03d} | Train Loss1: {loss1:.4f} | Loss2: {loss2:.4f} | Total: {train_loss:.4f} || \"\n",
        "                  f\"Val Loss1: {val_loss1:.4f} || Acc1: {acc1:.4f} | AUPRC1: {auprc1:.4f}\")\n",
        "\n",
        "            if val_loss1 < best_val_loss:\n",
        "                best_val_loss = val_loss1\n",
        "                best_model_state = model.state_dict()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            if patience_counter >= 40:\n",
        "                break\n",
        "\n",
        "        model.load_state_dict(best_model_state)\n",
        "        acc1, auprc1, _ = evaluate_y1_only(model, data_new, data_new.test_mask, loss_fn)\n",
        "        print(f\"\\n‚úÖ Test Accuracy: {acc1:.4f} | AUPRC1: {auprc1:.4f}\")\n",
        "        results.append((acc1, auprc1))\n",
        "\n",
        "    acc1s, auprc1s = zip(*results)\n",
        "    print(\"\\n===== T·ªïng k·∫øt sau 5 fold =====\")\n",
        "    print(f\"[Label 1] Accuracy: {np.mean(acc1s):.4f} ¬± {np.std(acc1s):.4f}\")\n",
        "    print(f\"[Label 1] AUPRC:   {np.mean(auprc1s):.4f} ¬± {np.std(auprc1s):.4f}\")\n",
        "run_kfold_training_with_pretrained_encoder(\n",
        "    data,\n",
        "    pretrained_encoder_path=\"/content/drive/MyDrive/Pretrained_Encoders/BRCA_encoder_epoch150.pth\",\n",
        "    hidden_channels=32\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P82LIqWNlrD",
        "outputId": "bbf4a175-9af6-4040-bc0d-e8fca7b06b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded pretrained encoder t·ª´: /content/drive/MyDrive/Pretrained_Encoders/BRCA_encoder_epoch150.pth\n",
            "\n",
            "üìÅ Fold 1/5\n",
            "Epoch 001 | Train Loss1: 0.7301 | Loss2: 0.6855 | Total: 0.7167 || Val Loss1: 0.7459 || Acc1: 0.1061 | AUPRC1: 0.5603\n",
            "Epoch 002 | Train Loss1: 0.6830 | Loss2: 0.6714 | Total: 0.6795 || Val Loss1: 0.6948 || Acc1: 0.6364 | AUPRC1: 0.5267\n",
            "Epoch 003 | Train Loss1: 0.6292 | Loss2: 0.6630 | Total: 0.6393 || Val Loss1: 0.6639 || Acc1: 0.8788 | AUPRC1: 0.2048\n",
            "Epoch 004 | Train Loss1: 0.5959 | Loss2: 0.6561 | Total: 0.6140 || Val Loss1: 0.6348 || Acc1: 0.8939 | AUPRC1: 0.1334\n",
            "Epoch 005 | Train Loss1: 0.5547 | Loss2: 0.6492 | Total: 0.5831 || Val Loss1: 0.5891 || Acc1: 0.8939 | AUPRC1: 0.2470\n",
            "Epoch 006 | Train Loss1: 0.4974 | Loss2: 0.6425 | Total: 0.5409 || Val Loss1: 0.5374 || Acc1: 0.8939 | AUPRC1: 0.4567\n",
            "Epoch 007 | Train Loss1: 0.4426 | Loss2: 0.6373 | Total: 0.5010 || Val Loss1: 0.4842 || Acc1: 0.8636 | AUPRC1: 0.4539\n",
            "Epoch 008 | Train Loss1: 0.3863 | Loss2: 0.6339 | Total: 0.4606 || Val Loss1: 0.4260 || Acc1: 0.9091 | AUPRC1: 0.4928\n",
            "Epoch 009 | Train Loss1: 0.3230 | Loss2: 0.6313 | Total: 0.4155 || Val Loss1: 0.3873 || Acc1: 0.9242 | AUPRC1: 0.5206\n",
            "Epoch 010 | Train Loss1: 0.2662 | Loss2: 0.6305 | Total: 0.3755 || Val Loss1: 0.3630 || Acc1: 0.9091 | AUPRC1: 0.4587\n",
            "Epoch 011 | Train Loss1: 0.2237 | Loss2: 0.6306 | Total: 0.3457 || Val Loss1: 0.3171 || Acc1: 0.9394 | AUPRC1: 0.5214\n",
            "Epoch 012 | Train Loss1: 0.1865 | Loss2: 0.6301 | Total: 0.3196 || Val Loss1: 0.2762 || Acc1: 0.8636 | AUPRC1: 0.5711\n",
            "Epoch 013 | Train Loss1: 0.1635 | Loss2: 0.6282 | Total: 0.3029 || Val Loss1: 0.2644 || Acc1: 0.8636 | AUPRC1: 0.6174\n",
            "Epoch 014 | Train Loss1: 0.1524 | Loss2: 0.6262 | Total: 0.2946 || Val Loss1: 0.2623 || Acc1: 0.8939 | AUPRC1: 0.6036\n",
            "Epoch 015 | Train Loss1: 0.1412 | Loss2: 0.6248 | Total: 0.2863 || Val Loss1: 0.2811 || Acc1: 0.9242 | AUPRC1: 0.5614\n",
            "Epoch 016 | Train Loss1: 0.1371 | Loss2: 0.6242 | Total: 0.2833 || Val Loss1: 0.2787 || Acc1: 0.9242 | AUPRC1: 0.5859\n",
            "Epoch 017 | Train Loss1: 0.1331 | Loss2: 0.6246 | Total: 0.2805 || Val Loss1: 0.2630 || Acc1: 0.8939 | AUPRC1: 0.6494\n",
            "Epoch 018 | Train Loss1: 0.1327 | Loss2: 0.6247 | Total: 0.2803 || Val Loss1: 0.2577 || Acc1: 0.8939 | AUPRC1: 0.6018\n",
            "Epoch 019 | Train Loss1: 0.1338 | Loss2: 0.6236 | Total: 0.2807 || Val Loss1: 0.2641 || Acc1: 0.9242 | AUPRC1: 0.5968\n",
            "Epoch 020 | Train Loss1: 0.1403 | Loss2: 0.6227 | Total: 0.2850 || Val Loss1: 0.2419 || Acc1: 0.8788 | AUPRC1: 0.6514\n",
            "Epoch 021 | Train Loss1: 0.1402 | Loss2: 0.6223 | Total: 0.2848 || Val Loss1: 0.2347 || Acc1: 0.8788 | AUPRC1: 0.6584\n",
            "Epoch 022 | Train Loss1: 0.1396 | Loss2: 0.6215 | Total: 0.2842 || Val Loss1: 0.2418 || Acc1: 0.9091 | AUPRC1: 0.6469\n",
            "Epoch 023 | Train Loss1: 0.1417 | Loss2: 0.6203 | Total: 0.2853 || Val Loss1: 0.2283 || Acc1: 0.9091 | AUPRC1: 0.6350\n",
            "Epoch 024 | Train Loss1: 0.1355 | Loss2: 0.6200 | Total: 0.2808 || Val Loss1: 0.2315 || Acc1: 0.9091 | AUPRC1: 0.6250\n",
            "Epoch 025 | Train Loss1: 0.1328 | Loss2: 0.6197 | Total: 0.2788 || Val Loss1: 0.2579 || Acc1: 0.9394 | AUPRC1: 0.5994\n",
            "Epoch 026 | Train Loss1: 0.1298 | Loss2: 0.6188 | Total: 0.2765 || Val Loss1: 0.2568 || Acc1: 0.9394 | AUPRC1: 0.5943\n",
            "Epoch 027 | Train Loss1: 0.1272 | Loss2: 0.6178 | Total: 0.2744 || Val Loss1: 0.2207 || Acc1: 0.9394 | AUPRC1: 0.6108\n",
            "Epoch 028 | Train Loss1: 0.1229 | Loss2: 0.6174 | Total: 0.2713 || Val Loss1: 0.2034 || Acc1: 0.9242 | AUPRC1: 0.6242\n",
            "Epoch 029 | Train Loss1: 0.1201 | Loss2: 0.6170 | Total: 0.2692 || Val Loss1: 0.2029 || Acc1: 0.9394 | AUPRC1: 0.6242\n",
            "Epoch 030 | Train Loss1: 0.1163 | Loss2: 0.6163 | Total: 0.2663 || Val Loss1: 0.1989 || Acc1: 0.9394 | AUPRC1: 0.6834\n",
            "Epoch 031 | Train Loss1: 0.1143 | Loss2: 0.6160 | Total: 0.2648 || Val Loss1: 0.1801 || Acc1: 0.9242 | AUPRC1: 0.6968\n",
            "Epoch 032 | Train Loss1: 0.1107 | Loss2: 0.6158 | Total: 0.2622 || Val Loss1: 0.1748 || Acc1: 0.9091 | AUPRC1: 0.7445\n",
            "Epoch 033 | Train Loss1: 0.1088 | Loss2: 0.6154 | Total: 0.2607 || Val Loss1: 0.1816 || Acc1: 0.9394 | AUPRC1: 0.6936\n",
            "Epoch 034 | Train Loss1: 0.1061 | Loss2: 0.6149 | Total: 0.2588 || Val Loss1: 0.1898 || Acc1: 0.9394 | AUPRC1: 0.6794\n",
            "Epoch 035 | Train Loss1: 0.1051 | Loss2: 0.6147 | Total: 0.2580 || Val Loss1: 0.1809 || Acc1: 0.9394 | AUPRC1: 0.6867\n",
            "Epoch 036 | Train Loss1: 0.1031 | Loss2: 0.6144 | Total: 0.2565 || Val Loss1: 0.1739 || Acc1: 0.9394 | AUPRC1: 0.6867\n",
            "Epoch 037 | Train Loss1: 0.1025 | Loss2: 0.6141 | Total: 0.2560 || Val Loss1: 0.1799 || Acc1: 0.9394 | AUPRC1: 0.6781\n",
            "Epoch 038 | Train Loss1: 0.1011 | Loss2: 0.6138 | Total: 0.2549 || Val Loss1: 0.1930 || Acc1: 0.9242 | AUPRC1: 0.6197\n",
            "Epoch 039 | Train Loss1: 0.1008 | Loss2: 0.6135 | Total: 0.2546 || Val Loss1: 0.1894 || Acc1: 0.9242 | AUPRC1: 0.6272\n",
            "Epoch 040 | Train Loss1: 0.1001 | Loss2: 0.6132 | Total: 0.2541 || Val Loss1: 0.1783 || Acc1: 0.9394 | AUPRC1: 0.6867\n",
            "Epoch 041 | Train Loss1: 0.0998 | Loss2: 0.6129 | Total: 0.2537 || Val Loss1: 0.1779 || Acc1: 0.9394 | AUPRC1: 0.6958\n",
            "Epoch 042 | Train Loss1: 0.0990 | Loss2: 0.6124 | Total: 0.2530 || Val Loss1: 0.1862 || Acc1: 0.9242 | AUPRC1: 0.6858\n",
            "Epoch 043 | Train Loss1: 0.0981 | Loss2: 0.6120 | Total: 0.2523 || Val Loss1: 0.1843 || Acc1: 0.9242 | AUPRC1: 0.6858\n",
            "Epoch 044 | Train Loss1: 0.0972 | Loss2: 0.6118 | Total: 0.2516 || Val Loss1: 0.1729 || Acc1: 0.9394 | AUPRC1: 0.6968\n",
            "Epoch 045 | Train Loss1: 0.0964 | Loss2: 0.6115 | Total: 0.2509 || Val Loss1: 0.1701 || Acc1: 0.9394 | AUPRC1: 0.7445\n",
            "Epoch 046 | Train Loss1: 0.0957 | Loss2: 0.6112 | Total: 0.2504 || Val Loss1: 0.1759 || Acc1: 0.9242 | AUPRC1: 0.6968\n",
            "Epoch 047 | Train Loss1: 0.0950 | Loss2: 0.6108 | Total: 0.2497 || Val Loss1: 0.1774 || Acc1: 0.9242 | AUPRC1: 0.7033\n",
            "Epoch 048 | Train Loss1: 0.0945 | Loss2: 0.6104 | Total: 0.2493 || Val Loss1: 0.1691 || Acc1: 0.9242 | AUPRC1: 0.7590\n",
            "Epoch 049 | Train Loss1: 0.0937 | Loss2: 0.6101 | Total: 0.2486 || Val Loss1: 0.1649 || Acc1: 0.9394 | AUPRC1: 0.7862\n",
            "Epoch 050 | Train Loss1: 0.0931 | Loss2: 0.6098 | Total: 0.2481 || Val Loss1: 0.1705 || Acc1: 0.9242 | AUPRC1: 0.7332\n",
            "Epoch 051 | Train Loss1: 0.0923 | Loss2: 0.6095 | Total: 0.2475 || Val Loss1: 0.1750 || Acc1: 0.9242 | AUPRC1: 0.7125\n",
            "Epoch 052 | Train Loss1: 0.0918 | Loss2: 0.6096 | Total: 0.2471 || Val Loss1: 0.1712 || Acc1: 0.9242 | AUPRC1: 0.7431\n",
            "Epoch 053 | Train Loss1: 0.0911 | Loss2: 0.6098 | Total: 0.2467 || Val Loss1: 0.1669 || Acc1: 0.9394 | AUPRC1: 0.7853\n",
            "Epoch 054 | Train Loss1: 0.0907 | Loss2: 0.6095 | Total: 0.2463 || Val Loss1: 0.1719 || Acc1: 0.9394 | AUPRC1: 0.7149\n",
            "Epoch 055 | Train Loss1: 0.0900 | Loss2: 0.6088 | Total: 0.2456 || Val Loss1: 0.1749 || Acc1: 0.9394 | AUPRC1: 0.7149\n",
            "Epoch 056 | Train Loss1: 0.0895 | Loss2: 0.6074 | Total: 0.2449 || Val Loss1: 0.1688 || Acc1: 0.9394 | AUPRC1: 0.7626\n",
            "Epoch 057 | Train Loss1: 0.0889 | Loss2: 0.6071 | Total: 0.2443 || Val Loss1: 0.1650 || Acc1: 0.9394 | AUPRC1: 0.7804\n",
            "Epoch 058 | Train Loss1: 0.0884 | Loss2: 0.6076 | Total: 0.2441 || Val Loss1: 0.1672 || Acc1: 0.9394 | AUPRC1: 0.7726\n",
            "Epoch 059 | Train Loss1: 0.0878 | Loss2: 0.6071 | Total: 0.2436 || Val Loss1: 0.1696 || Acc1: 0.9394 | AUPRC1: 0.7369\n",
            "Epoch 060 | Train Loss1: 0.0873 | Loss2: 0.6060 | Total: 0.2429 || Val Loss1: 0.1643 || Acc1: 0.9394 | AUPRC1: 0.7931\n",
            "Epoch 061 | Train Loss1: 0.0866 | Loss2: 0.6055 | Total: 0.2423 || Val Loss1: 0.1629 || Acc1: 0.9394 | AUPRC1: 0.7931\n",
            "Epoch 062 | Train Loss1: 0.0861 | Loss2: 0.6056 | Total: 0.2420 || Val Loss1: 0.1679 || Acc1: 0.9394 | AUPRC1: 0.7853\n",
            "Epoch 063 | Train Loss1: 0.0856 | Loss2: 0.6057 | Total: 0.2416 || Val Loss1: 0.1644 || Acc1: 0.9394 | AUPRC1: 0.7853\n",
            "Epoch 064 | Train Loss1: 0.0850 | Loss2: 0.6050 | Total: 0.2410 || Val Loss1: 0.1602 || Acc1: 0.9394 | AUPRC1: 0.8026\n",
            "Epoch 065 | Train Loss1: 0.0844 | Loss2: 0.6043 | Total: 0.2404 || Val Loss1: 0.1604 || Acc1: 0.9394 | AUPRC1: 0.7899\n",
            "Epoch 066 | Train Loss1: 0.0838 | Loss2: 0.6040 | Total: 0.2398 || Val Loss1: 0.1623 || Acc1: 0.9394 | AUPRC1: 0.7899\n",
            "Epoch 067 | Train Loss1: 0.0833 | Loss2: 0.6039 | Total: 0.2395 || Val Loss1: 0.1593 || Acc1: 0.9394 | AUPRC1: 0.7899\n",
            "Epoch 068 | Train Loss1: 0.0828 | Loss2: 0.6039 | Total: 0.2391 || Val Loss1: 0.1562 || Acc1: 0.9394 | AUPRC1: 0.8145\n",
            "Epoch 069 | Train Loss1: 0.0823 | Loss2: 0.6036 | Total: 0.2387 || Val Loss1: 0.1632 || Acc1: 0.9242 | AUPRC1: 0.8026\n",
            "Epoch 070 | Train Loss1: 0.0818 | Loss2: 0.6033 | Total: 0.2383 || Val Loss1: 0.1573 || Acc1: 0.9242 | AUPRC1: 0.8145\n",
            "Epoch 071 | Train Loss1: 0.0812 | Loss2: 0.6031 | Total: 0.2378 || Val Loss1: 0.1561 || Acc1: 0.9242 | AUPRC1: 0.8145\n",
            "Epoch 072 | Train Loss1: 0.0806 | Loss2: 0.6029 | Total: 0.2373 || Val Loss1: 0.1579 || Acc1: 0.9242 | AUPRC1: 0.8145\n",
            "Epoch 073 | Train Loss1: 0.0800 | Loss2: 0.6020 | Total: 0.2366 || Val Loss1: 0.1556 || Acc1: 0.9242 | AUPRC1: 0.8145\n",
            "Epoch 074 | Train Loss1: 0.0794 | Loss2: 0.6012 | Total: 0.2360 || Val Loss1: 0.1511 || Acc1: 0.9394 | AUPRC1: 0.8145\n",
            "Epoch 075 | Train Loss1: 0.0789 | Loss2: 0.6006 | Total: 0.2354 || Val Loss1: 0.1548 || Acc1: 0.9394 | AUPRC1: 0.8145\n",
            "Epoch 076 | Train Loss1: 0.0783 | Loss2: 0.6004 | Total: 0.2349 || Val Loss1: 0.1500 || Acc1: 0.9394 | AUPRC1: 0.8145\n",
            "Epoch 077 | Train Loss1: 0.0776 | Loss2: 0.6004 | Total: 0.2345 || Val Loss1: 0.1482 || Acc1: 0.9394 | AUPRC1: 0.8145\n",
            "Epoch 078 | Train Loss1: 0.0771 | Loss2: 0.6011 | Total: 0.2343 || Val Loss1: 0.1549 || Acc1: 0.9394 | AUPRC1: 0.8145\n",
            "Epoch 079 | Train Loss1: 0.0765 | Loss2: 0.6027 | Total: 0.2344 || Val Loss1: 0.1436 || Acc1: 0.9394 | AUPRC1: 0.8145\n",
            "Epoch 080 | Train Loss1: 0.0759 | Loss2: 0.6032 | Total: 0.2341 || Val Loss1: 0.1506 || Acc1: 0.9394 | AUPRC1: 0.8145\n",
            "Epoch 081 | Train Loss1: 0.0752 | Loss2: 0.6014 | Total: 0.2331 || Val Loss1: 0.1428 || Acc1: 0.9394 | AUPRC1: 0.8145\n",
            "Epoch 082 | Train Loss1: 0.0745 | Loss2: 0.5989 | Total: 0.2318 || Val Loss1: 0.1457 || Acc1: 0.9394 | AUPRC1: 0.8145\n",
            "Epoch 083 | Train Loss1: 0.0738 | Loss2: 0.5999 | Total: 0.2316 || Val Loss1: 0.1488 || Acc1: 0.9394 | AUPRC1: 0.8145\n",
            "Epoch 084 | Train Loss1: 0.0733 | Loss2: 0.6014 | Total: 0.2317 || Val Loss1: 0.1362 || Acc1: 0.9394 | AUPRC1: 0.8187\n",
            "Epoch 085 | Train Loss1: 0.0728 | Loss2: 0.5993 | Total: 0.2308 || Val Loss1: 0.1547 || Acc1: 0.9394 | AUPRC1: 0.8187\n",
            "Epoch 086 | Train Loss1: 0.0724 | Loss2: 0.5978 | Total: 0.2300 || Val Loss1: 0.1341 || Acc1: 0.9394 | AUPRC1: 0.8290\n",
            "Epoch 087 | Train Loss1: 0.0714 | Loss2: 0.5987 | Total: 0.2296 || Val Loss1: 0.1393 || Acc1: 0.9394 | AUPRC1: 0.8235\n",
            "Epoch 088 | Train Loss1: 0.0705 | Loss2: 0.5990 | Total: 0.2291 || Val Loss1: 0.1461 || Acc1: 0.9394 | AUPRC1: 0.8235\n",
            "Epoch 089 | Train Loss1: 0.0701 | Loss2: 0.5980 | Total: 0.2285 || Val Loss1: 0.1269 || Acc1: 0.9394 | AUPRC1: 0.8290\n",
            "Epoch 090 | Train Loss1: 0.0699 | Loss2: 0.5970 | Total: 0.2280 || Val Loss1: 0.1453 || Acc1: 0.9394 | AUPRC1: 0.8558\n",
            "Epoch 091 | Train Loss1: 0.0690 | Loss2: 0.5969 | Total: 0.2274 || Val Loss1: 0.1337 || Acc1: 0.9394 | AUPRC1: 0.8613\n",
            "Epoch 092 | Train Loss1: 0.0680 | Loss2: 0.5970 | Total: 0.2267 || Val Loss1: 0.1214 || Acc1: 0.9394 | AUPRC1: 0.8613\n",
            "Epoch 093 | Train Loss1: 0.0677 | Loss2: 0.5967 | Total: 0.2264 || Val Loss1: 0.1428 || Acc1: 0.9242 | AUPRC1: 0.8613\n",
            "Epoch 094 | Train Loss1: 0.0675 | Loss2: 0.5962 | Total: 0.2261 || Val Loss1: 0.1142 || Acc1: 0.9394 | AUPRC1: 0.8677\n",
            "Epoch 095 | Train Loss1: 0.0669 | Loss2: 0.5958 | Total: 0.2256 || Val Loss1: 0.1323 || Acc1: 0.9394 | AUPRC1: 0.8613\n",
            "Epoch 096 | Train Loss1: 0.0660 | Loss2: 0.5954 | Total: 0.2248 || Val Loss1: 0.1195 || Acc1: 0.9394 | AUPRC1: 0.8613\n",
            "Epoch 097 | Train Loss1: 0.0651 | Loss2: 0.5952 | Total: 0.2241 || Val Loss1: 0.1179 || Acc1: 0.9394 | AUPRC1: 0.8613\n",
            "Epoch 098 | Train Loss1: 0.0646 | Loss2: 0.5949 | Total: 0.2237 || Val Loss1: 0.1336 || Acc1: 0.9394 | AUPRC1: 0.8613\n",
            "Epoch 099 | Train Loss1: 0.0644 | Loss2: 0.5947 | Total: 0.2235 || Val Loss1: 0.1108 || Acc1: 0.9394 | AUPRC1: 0.8963\n",
            "Epoch 100 | Train Loss1: 0.0641 | Loss2: 0.5948 | Total: 0.2233 || Val Loss1: 0.1337 || Acc1: 0.9394 | AUPRC1: 0.8613\n",
            "Epoch 101 | Train Loss1: 0.0637 | Loss2: 0.5948 | Total: 0.2230 || Val Loss1: 0.1083 || Acc1: 0.9394 | AUPRC1: 0.8963\n",
            "Epoch 102 | Train Loss1: 0.0630 | Loss2: 0.5952 | Total: 0.2227 || Val Loss1: 0.1256 || Acc1: 0.9394 | AUPRC1: 0.8613\n",
            "Epoch 103 | Train Loss1: 0.0623 | Loss2: 0.5965 | Total: 0.2225 || Val Loss1: 0.1143 || Acc1: 0.9394 | AUPRC1: 0.8613\n",
            "Epoch 104 | Train Loss1: 0.0615 | Loss2: 0.5975 | Total: 0.2223 || Val Loss1: 0.1146 || Acc1: 0.9394 | AUPRC1: 0.8613\n",
            "Epoch 105 | Train Loss1: 0.0610 | Loss2: 0.5982 | Total: 0.2222 || Val Loss1: 0.1193 || Acc1: 0.9394 | AUPRC1: 0.8613\n",
            "Epoch 106 | Train Loss1: 0.0607 | Loss2: 0.5944 | Total: 0.2208 || Val Loss1: 0.1052 || Acc1: 0.9394 | AUPRC1: 0.8753\n",
            "Epoch 107 | Train Loss1: 0.0603 | Loss2: 0.5935 | Total: 0.2202 || Val Loss1: 0.1161 || Acc1: 0.9394 | AUPRC1: 0.8677\n",
            "Epoch 108 | Train Loss1: 0.0600 | Loss2: 0.5959 | Total: 0.2208 || Val Loss1: 0.1002 || Acc1: 0.9545 | AUPRC1: 0.9038\n",
            "Epoch 109 | Train Loss1: 0.0599 | Loss2: 0.5958 | Total: 0.2206 || Val Loss1: 0.1315 || Acc1: 0.9394 | AUPRC1: 0.8486\n",
            "Epoch 110 | Train Loss1: 0.0601 | Loss2: 0.5936 | Total: 0.2201 || Val Loss1: 0.0974 || Acc1: 0.9697 | AUPRC1: 0.9129\n",
            "Epoch 111 | Train Loss1: 0.0604 | Loss2: 0.5926 | Total: 0.2201 || Val Loss1: 0.1428 || Acc1: 0.9091 | AUPRC1: 0.8486\n",
            "Epoch 112 | Train Loss1: 0.0605 | Loss2: 0.5937 | Total: 0.2204 || Val Loss1: 0.0958 || Acc1: 0.9545 | AUPRC1: 0.9129\n",
            "Epoch 113 | Train Loss1: 0.0586 | Loss2: 0.5941 | Total: 0.2192 || Val Loss1: 0.1074 || Acc1: 0.9394 | AUPRC1: 0.9129\n",
            "Epoch 114 | Train Loss1: 0.0570 | Loss2: 0.5923 | Total: 0.2176 || Val Loss1: 0.1142 || Acc1: 0.9394 | AUPRC1: 0.9129\n",
            "Epoch 115 | Train Loss1: 0.0569 | Loss2: 0.5921 | Total: 0.2175 || Val Loss1: 0.0909 || Acc1: 0.9697 | AUPRC1: 0.9367\n",
            "Epoch 116 | Train Loss1: 0.0575 | Loss2: 0.5931 | Total: 0.2182 || Val Loss1: 0.1285 || Acc1: 0.9394 | AUPRC1: 0.9129\n",
            "Epoch 117 | Train Loss1: 0.0576 | Loss2: 0.5920 | Total: 0.2179 || Val Loss1: 0.0906 || Acc1: 0.9545 | AUPRC1: 0.9367\n",
            "Epoch 118 | Train Loss1: 0.0563 | Loss2: 0.5915 | Total: 0.2169 || Val Loss1: 0.1070 || Acc1: 0.9545 | AUPRC1: 0.9240\n",
            "Epoch 119 | Train Loss1: 0.0551 | Loss2: 0.5925 | Total: 0.2163 || Val Loss1: 0.0980 || Acc1: 0.9545 | AUPRC1: 0.9478\n",
            "Epoch 120 | Train Loss1: 0.0545 | Loss2: 0.5929 | Total: 0.2160 || Val Loss1: 0.0869 || Acc1: 0.9697 | AUPRC1: 0.9478\n",
            "Epoch 121 | Train Loss1: 0.0544 | Loss2: 0.5918 | Total: 0.2156 || Val Loss1: 0.1068 || Acc1: 0.9545 | AUPRC1: 0.9240\n",
            "Epoch 122 | Train Loss1: 0.0543 | Loss2: 0.5908 | Total: 0.2153 || Val Loss1: 0.0834 || Acc1: 0.9848 | AUPRC1: 0.9571\n",
            "Epoch 123 | Train Loss1: 0.0542 | Loss2: 0.5918 | Total: 0.2155 || Val Loss1: 0.1138 || Acc1: 0.9545 | AUPRC1: 0.9129\n",
            "Epoch 124 | Train Loss1: 0.0542 | Loss2: 0.5932 | Total: 0.2159 || Val Loss1: 0.0815 || Acc1: 0.9848 | AUPRC1: 0.9683\n",
            "Epoch 125 | Train Loss1: 0.0539 | Loss2: 0.5920 | Total: 0.2153 || Val Loss1: 0.1076 || Acc1: 0.9545 | AUPRC1: 0.9240\n",
            "Epoch 126 | Train Loss1: 0.0533 | Loss2: 0.5903 | Total: 0.2144 || Val Loss1: 0.0799 || Acc1: 0.9848 | AUPRC1: 0.9683\n",
            "Epoch 127 | Train Loss1: 0.0523 | Loss2: 0.5904 | Total: 0.2137 || Val Loss1: 0.0873 || Acc1: 0.9697 | AUPRC1: 0.9683\n",
            "Epoch 128 | Train Loss1: 0.0516 | Loss2: 0.5906 | Total: 0.2133 || Val Loss1: 0.0901 || Acc1: 0.9545 | AUPRC1: 0.9683\n",
            "Epoch 129 | Train Loss1: 0.0514 | Loss2: 0.5902 | Total: 0.2130 || Val Loss1: 0.0757 || Acc1: 0.9848 | AUPRC1: 0.9683\n",
            "Epoch 130 | Train Loss1: 0.0517 | Loss2: 0.5895 | Total: 0.2130 || Val Loss1: 0.1087 || Acc1: 0.9545 | AUPRC1: 0.9240\n",
            "Epoch 131 | Train Loss1: 0.0524 | Loss2: 0.5893 | Total: 0.2135 || Val Loss1: 0.0725 || Acc1: 0.9848 | AUPRC1: 0.9821\n",
            "Epoch 132 | Train Loss1: 0.0529 | Loss2: 0.5894 | Total: 0.2139 || Val Loss1: 0.1161 || Acc1: 0.9394 | AUPRC1: 0.9240\n",
            "Epoch 133 | Train Loss1: 0.0533 | Loss2: 0.5890 | Total: 0.2140 || Val Loss1: 0.0689 || Acc1: 0.9848 | AUPRC1: 0.9821\n",
            "Epoch 134 | Train Loss1: 0.0522 | Loss2: 0.5887 | Total: 0.2131 || Val Loss1: 0.0971 || Acc1: 0.9545 | AUPRC1: 0.9478\n",
            "Epoch 135 | Train Loss1: 0.0508 | Loss2: 0.5888 | Total: 0.2122 || Val Loss1: 0.0720 || Acc1: 0.9848 | AUPRC1: 0.9821\n",
            "Epoch 136 | Train Loss1: 0.0493 | Loss2: 0.5892 | Total: 0.2113 || Val Loss1: 0.0715 || Acc1: 0.9848 | AUPRC1: 0.9821\n",
            "Epoch 137 | Train Loss1: 0.0491 | Loss2: 0.5891 | Total: 0.2111 || Val Loss1: 0.0902 || Acc1: 0.9545 | AUPRC1: 0.9683\n",
            "Epoch 138 | Train Loss1: 0.0495 | Loss2: 0.5882 | Total: 0.2111 || Val Loss1: 0.0659 || Acc1: 0.9848 | AUPRC1: 0.9821\n",
            "Epoch 139 | Train Loss1: 0.0499 | Loss2: 0.5878 | Total: 0.2113 || Val Loss1: 0.0956 || Acc1: 0.9545 | AUPRC1: 0.9683\n",
            "Epoch 140 | Train Loss1: 0.0500 | Loss2: 0.5882 | Total: 0.2114 || Val Loss1: 0.0638 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 141 | Train Loss1: 0.0494 | Loss2: 0.5889 | Total: 0.2113 || Val Loss1: 0.0854 || Acc1: 0.9545 | AUPRC1: 0.9683\n",
            "Epoch 142 | Train Loss1: 0.0485 | Loss2: 0.5891 | Total: 0.2107 || Val Loss1: 0.0666 || Acc1: 0.9848 | AUPRC1: 0.9821\n",
            "Epoch 143 | Train Loss1: 0.0475 | Loss2: 0.5881 | Total: 0.2097 || Val Loss1: 0.0685 || Acc1: 0.9848 | AUPRC1: 0.9821\n",
            "Epoch 144 | Train Loss1: 0.0471 | Loss2: 0.5870 | Total: 0.2091 || Val Loss1: 0.0770 || Acc1: 0.9697 | AUPRC1: 0.9683\n",
            "Epoch 145 | Train Loss1: 0.0472 | Loss2: 0.5868 | Total: 0.2091 || Val Loss1: 0.0619 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 146 | Train Loss1: 0.0474 | Loss2: 0.5874 | Total: 0.2094 || Val Loss1: 0.0841 || Acc1: 0.9545 | AUPRC1: 0.9683\n",
            "Epoch 147 | Train Loss1: 0.0477 | Loss2: 0.5880 | Total: 0.2098 || Val Loss1: 0.0590 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 148 | Train Loss1: 0.0478 | Loss2: 0.5878 | Total: 0.2098 || Val Loss1: 0.0849 || Acc1: 0.9545 | AUPRC1: 0.9683\n",
            "Epoch 149 | Train Loss1: 0.0476 | Loss2: 0.5870 | Total: 0.2094 || Val Loss1: 0.0579 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 150 | Train Loss1: 0.0467 | Loss2: 0.5861 | Total: 0.2085 || Val Loss1: 0.0711 || Acc1: 0.9848 | AUPRC1: 0.9821\n",
            "Epoch 151 | Train Loss1: 0.0458 | Loss2: 0.5857 | Total: 0.2078 || Val Loss1: 0.0634 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 152 | Train Loss1: 0.0453 | Loss2: 0.5860 | Total: 0.2075 || Val Loss1: 0.0582 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 153 | Train Loss1: 0.0454 | Loss2: 0.5862 | Total: 0.2076 || Val Loss1: 0.0788 || Acc1: 0.9545 | AUPRC1: 0.9821\n",
            "Epoch 154 | Train Loss1: 0.0462 | Loss2: 0.5862 | Total: 0.2082 || Val Loss1: 0.0563 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 155 | Train Loss1: 0.0473 | Loss2: 0.5859 | Total: 0.2089 || Val Loss1: 0.1040 || Acc1: 0.9394 | AUPRC1: 0.9683\n",
            "Epoch 156 | Train Loss1: 0.0487 | Loss2: 0.5853 | Total: 0.2097 || Val Loss1: 0.0561 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 157 | Train Loss1: 0.0491 | Loss2: 0.5849 | Total: 0.2098 || Val Loss1: 0.0923 || Acc1: 0.9545 | AUPRC1: 0.9683\n",
            "Epoch 158 | Train Loss1: 0.0471 | Loss2: 0.5847 | Total: 0.2084 || Val Loss1: 0.0574 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 159 | Train Loss1: 0.0442 | Loss2: 0.5847 | Total: 0.2064 || Val Loss1: 0.0536 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 160 | Train Loss1: 0.0450 | Loss2: 0.5849 | Total: 0.2069 || Val Loss1: 0.0952 || Acc1: 0.9394 | AUPRC1: 0.9821\n",
            "Epoch 161 | Train Loss1: 0.0474 | Loss2: 0.5850 | Total: 0.2087 || Val Loss1: 0.0542 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 162 | Train Loss1: 0.0471 | Loss2: 0.5846 | Total: 0.2084 || Val Loss1: 0.0797 || Acc1: 0.9545 | AUPRC1: 1.0000\n",
            "Epoch 163 | Train Loss1: 0.0447 | Loss2: 0.5839 | Total: 0.2064 || Val Loss1: 0.0613 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 164 | Train Loss1: 0.0432 | Loss2: 0.5836 | Total: 0.2053 || Val Loss1: 0.0499 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 165 | Train Loss1: 0.0447 | Loss2: 0.5837 | Total: 0.2064 || Val Loss1: 0.0841 || Acc1: 0.9394 | AUPRC1: 1.0000\n",
            "Epoch 166 | Train Loss1: 0.0459 | Loss2: 0.5839 | Total: 0.2073 || Val Loss1: 0.0491 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 167 | Train Loss1: 0.0442 | Loss2: 0.5838 | Total: 0.2061 || Val Loss1: 0.0599 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 168 | Train Loss1: 0.0424 | Loss2: 0.5837 | Total: 0.2048 || Val Loss1: 0.0684 || Acc1: 0.9697 | AUPRC1: 1.0000\n",
            "Epoch 169 | Train Loss1: 0.0427 | Loss2: 0.5834 | Total: 0.2049 || Val Loss1: 0.0510 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 170 | Train Loss1: 0.0440 | Loss2: 0.5833 | Total: 0.2058 || Val Loss1: 0.0814 || Acc1: 0.9545 | AUPRC1: 1.0000\n",
            "Epoch 171 | Train Loss1: 0.0442 | Loss2: 0.5829 | Total: 0.2058 || Val Loss1: 0.0486 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 172 | Train Loss1: 0.0424 | Loss2: 0.5826 | Total: 0.2045 || Val Loss1: 0.0515 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 173 | Train Loss1: 0.0415 | Loss2: 0.5822 | Total: 0.2037 || Val Loss1: 0.0658 || Acc1: 0.9697 | AUPRC1: 1.0000\n",
            "Epoch 174 | Train Loss1: 0.0423 | Loss2: 0.5821 | Total: 0.2042 || Val Loss1: 0.0477 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 175 | Train Loss1: 0.0428 | Loss2: 0.5819 | Total: 0.2045 || Val Loss1: 0.0691 || Acc1: 0.9697 | AUPRC1: 1.0000\n",
            "Epoch 176 | Train Loss1: 0.0421 | Loss2: 0.5817 | Total: 0.2040 || Val Loss1: 0.0509 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 177 | Train Loss1: 0.0410 | Loss2: 0.5816 | Total: 0.2032 || Val Loss1: 0.0510 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 178 | Train Loss1: 0.0406 | Loss2: 0.5815 | Total: 0.2029 || Val Loss1: 0.0624 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 179 | Train Loss1: 0.0412 | Loss2: 0.5814 | Total: 0.2032 || Val Loss1: 0.0457 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 180 | Train Loss1: 0.0415 | Loss2: 0.5816 | Total: 0.2035 || Val Loss1: 0.0645 || Acc1: 0.9697 | AUPRC1: 1.0000\n",
            "Epoch 181 | Train Loss1: 0.0411 | Loss2: 0.5821 | Total: 0.2034 || Val Loss1: 0.0479 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 182 | Train Loss1: 0.0402 | Loss2: 0.5835 | Total: 0.2032 || Val Loss1: 0.0522 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 183 | Train Loss1: 0.0397 | Loss2: 0.5864 | Total: 0.2037 || Val Loss1: 0.0573 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 184 | Train Loss1: 0.0399 | Loss2: 0.5877 | Total: 0.2042 || Val Loss1: 0.0455 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 185 | Train Loss1: 0.0400 | Loss2: 0.5872 | Total: 0.2042 || Val Loss1: 0.0585 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 186 | Train Loss1: 0.0400 | Loss2: 0.5815 | Total: 0.2024 || Val Loss1: 0.0452 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 187 | Train Loss1: 0.0395 | Loss2: 0.5812 | Total: 0.2020 || Val Loss1: 0.0555 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 188 | Train Loss1: 0.0391 | Loss2: 0.5860 | Total: 0.2032 || Val Loss1: 0.0486 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 189 | Train Loss1: 0.0388 | Loss2: 0.5859 | Total: 0.2029 || Val Loss1: 0.0529 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 190 | Train Loss1: 0.0386 | Loss2: 0.5825 | Total: 0.2017 || Val Loss1: 0.0484 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 191 | Train Loss1: 0.0384 | Loss2: 0.5798 | Total: 0.2008 || Val Loss1: 0.0482 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 192 | Train Loss1: 0.0382 | Loss2: 0.5817 | Total: 0.2012 || Val Loss1: 0.0478 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 193 | Train Loss1: 0.0380 | Loss2: 0.5838 | Total: 0.2017 || Val Loss1: 0.0443 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 194 | Train Loss1: 0.0380 | Loss2: 0.5811 | Total: 0.2009 || Val Loss1: 0.0592 || Acc1: 0.9697 | AUPRC1: 1.0000\n",
            "Epoch 195 | Train Loss1: 0.0385 | Loss2: 0.5793 | Total: 0.2007 || Val Loss1: 0.0430 || Acc1: 1.0000 | AUPRC1: 1.0000\n",
            "Epoch 196 | Train Loss1: 0.0396 | Loss2: 0.5804 | Total: 0.2019 || Val Loss1: 0.0878 || Acc1: 0.9394 | AUPRC1: 1.0000\n",
            "Epoch 197 | Train Loss1: 0.0419 | Loss2: 0.5805 | Total: 0.2035 || Val Loss1: 0.0549 || Acc1: 0.9848 | AUPRC1: 1.0000\n",
            "Epoch 198 | Train Loss1: 0.0474 | Loss2: 0.5794 | Total: 0.2070 || Val Loss1: 0.1747 || Acc1: 0.9242 | AUPRC1: 0.8183\n",
            "Epoch 199 | Train Loss1: 0.0553 | Loss2: 0.5805 | Total: 0.2128 || Val Loss1: 0.0736 || Acc1: 0.9697 | AUPRC1: 1.0000\n",
            "Epoch 200 | Train Loss1: 0.0580 | Loss2: 0.5853 | Total: 0.2162 || Val Loss1: 0.1284 || Acc1: 0.9394 | AUPRC1: 0.9123\n",
            "\n",
            "‚úÖ Test Accuracy: 0.9398 | AUPRC1: 0.8132\n",
            "\n",
            "üìÅ Fold 2/5\n",
            "Epoch 001 | Train Loss1: 0.7340 | Loss2: 0.7089 | Total: 0.7264 || Val Loss1: 0.6845 || Acc1: 0.3333 | AUPRC1: 0.1146\n",
            "Epoch 002 | Train Loss1: 0.6988 | Loss2: 0.6898 | Total: 0.6961 || Val Loss1: 0.6740 || Acc1: 0.8636 | AUPRC1: 0.2720\n",
            "Epoch 003 | Train Loss1: 0.6294 | Loss2: 0.6804 | Total: 0.6447 || Val Loss1: 0.6659 || Acc1: 0.8333 | AUPRC1: 0.3557\n",
            "Epoch 004 | Train Loss1: 0.5994 | Loss2: 0.6706 | Total: 0.6207 || Val Loss1: 0.6444 || Acc1: 0.8333 | AUPRC1: 0.3430\n",
            "Epoch 005 | Train Loss1: 0.5688 | Loss2: 0.6613 | Total: 0.5965 || Val Loss1: 0.6092 || Acc1: 0.8485 | AUPRC1: 0.3109\n",
            "Epoch 006 | Train Loss1: 0.5336 | Loss2: 0.6542 | Total: 0.5698 || Val Loss1: 0.5702 || Acc1: 0.8636 | AUPRC1: 0.1987\n",
            "Epoch 007 | Train Loss1: 0.4979 | Loss2: 0.6489 | Total: 0.5432 || Val Loss1: 0.5352 || Acc1: 0.8788 | AUPRC1: 0.1629\n",
            "Epoch 008 | Train Loss1: 0.4588 | Loss2: 0.6435 | Total: 0.5142 || Val Loss1: 0.5001 || Acc1: 0.8788 | AUPRC1: 0.1621\n",
            "Epoch 009 | Train Loss1: 0.4123 | Loss2: 0.6394 | Total: 0.4804 || Val Loss1: 0.4626 || Acc1: 0.8788 | AUPRC1: 0.1813\n",
            "Epoch 010 | Train Loss1: 0.3639 | Loss2: 0.6373 | Total: 0.4459 || Val Loss1: 0.4219 || Acc1: 0.8788 | AUPRC1: 0.1993\n",
            "Epoch 011 | Train Loss1: 0.3139 | Loss2: 0.6346 | Total: 0.4101 || Val Loss1: 0.3831 || Acc1: 0.8788 | AUPRC1: 0.2278\n",
            "Epoch 012 | Train Loss1: 0.2637 | Loss2: 0.6337 | Total: 0.3747 || Val Loss1: 0.3478 || Acc1: 0.8788 | AUPRC1: 0.2510\n",
            "Epoch 013 | Train Loss1: 0.2169 | Loss2: 0.6325 | Total: 0.3416 || Val Loss1: 0.3157 || Acc1: 0.8788 | AUPRC1: 0.2851\n",
            "Epoch 014 | Train Loss1: 0.1767 | Loss2: 0.6327 | Total: 0.3135 || Val Loss1: 0.2891 || Acc1: 0.8788 | AUPRC1: 0.3673\n",
            "Epoch 015 | Train Loss1: 0.1455 | Loss2: 0.6313 | Total: 0.2913 || Val Loss1: 0.2806 || Acc1: 0.8788 | AUPRC1: 0.4603\n",
            "Epoch 016 | Train Loss1: 0.1250 | Loss2: 0.6300 | Total: 0.2765 || Val Loss1: 0.2738 || Acc1: 0.8939 | AUPRC1: 0.5132\n",
            "Epoch 017 | Train Loss1: 0.1159 | Loss2: 0.6275 | Total: 0.2694 || Val Loss1: 0.3023 || Acc1: 0.8939 | AUPRC1: 0.5247\n",
            "Epoch 018 | Train Loss1: 0.1106 | Loss2: 0.6255 | Total: 0.2651 || Val Loss1: 0.2677 || Acc1: 0.9091 | AUPRC1: 0.5088\n",
            "Epoch 019 | Train Loss1: 0.1091 | Loss2: 0.6253 | Total: 0.2640 || Val Loss1: 0.2810 || Acc1: 0.9091 | AUPRC1: 0.4981\n",
            "Epoch 020 | Train Loss1: 0.1068 | Loss2: 0.6244 | Total: 0.2621 || Val Loss1: 0.3016 || Acc1: 0.9091 | AUPRC1: 0.4902\n",
            "Epoch 021 | Train Loss1: 0.1081 | Loss2: 0.6237 | Total: 0.2628 || Val Loss1: 0.2948 || Acc1: 0.9091 | AUPRC1: 0.4694\n",
            "Epoch 022 | Train Loss1: 0.1093 | Loss2: 0.6235 | Total: 0.2635 || Val Loss1: 0.3050 || Acc1: 0.9091 | AUPRC1: 0.4966\n",
            "Epoch 023 | Train Loss1: 0.1065 | Loss2: 0.6224 | Total: 0.2613 || Val Loss1: 0.3182 || Acc1: 0.8788 | AUPRC1: 0.4694\n",
            "Epoch 024 | Train Loss1: 0.1058 | Loss2: 0.6215 | Total: 0.2605 || Val Loss1: 0.3211 || Acc1: 0.8939 | AUPRC1: 0.4365\n",
            "Epoch 025 | Train Loss1: 0.1084 | Loss2: 0.6208 | Total: 0.2621 || Val Loss1: 0.4197 || Acc1: 0.8636 | AUPRC1: 0.4526\n",
            "Epoch 026 | Train Loss1: 0.1167 | Loss2: 0.6196 | Total: 0.2676 || Val Loss1: 0.3255 || Acc1: 0.8939 | AUPRC1: 0.4210\n",
            "Epoch 027 | Train Loss1: 0.1044 | Loss2: 0.6185 | Total: 0.2586 || Val Loss1: 0.3213 || Acc1: 0.8788 | AUPRC1: 0.4011\n",
            "Epoch 028 | Train Loss1: 0.1013 | Loss2: 0.6182 | Total: 0.2564 || Val Loss1: 0.3608 || Acc1: 0.8636 | AUPRC1: 0.4467\n",
            "Epoch 029 | Train Loss1: 0.1030 | Loss2: 0.6183 | Total: 0.2576 || Val Loss1: 0.3081 || Acc1: 0.8788 | AUPRC1: 0.3985\n",
            "Epoch 030 | Train Loss1: 0.0964 | Loss2: 0.6176 | Total: 0.2528 || Val Loss1: 0.2885 || Acc1: 0.9091 | AUPRC1: 0.4758\n",
            "Epoch 031 | Train Loss1: 0.0981 | Loss2: 0.6164 | Total: 0.2536 || Val Loss1: 0.2987 || Acc1: 0.8939 | AUPRC1: 0.5121\n",
            "Epoch 032 | Train Loss1: 0.0937 | Loss2: 0.6154 | Total: 0.2502 || Val Loss1: 0.2928 || Acc1: 0.8939 | AUPRC1: 0.4495\n",
            "Epoch 033 | Train Loss1: 0.0927 | Loss2: 0.6150 | Total: 0.2494 || Val Loss1: 0.2694 || Acc1: 0.9091 | AUPRC1: 0.4561\n",
            "Epoch 034 | Train Loss1: 0.0927 | Loss2: 0.6151 | Total: 0.2494 || Val Loss1: 0.2734 || Acc1: 0.9091 | AUPRC1: 0.4746\n",
            "Epoch 035 | Train Loss1: 0.0885 | Loss2: 0.6156 | Total: 0.2466 || Val Loss1: 0.2902 || Acc1: 0.8939 | AUPRC1: 0.4507\n",
            "Epoch 036 | Train Loss1: 0.0887 | Loss2: 0.6157 | Total: 0.2468 || Val Loss1: 0.2720 || Acc1: 0.9091 | AUPRC1: 0.4324\n",
            "Epoch 037 | Train Loss1: 0.0864 | Loss2: 0.6144 | Total: 0.2448 || Val Loss1: 0.2716 || Acc1: 0.9091 | AUPRC1: 0.4369\n",
            "Epoch 038 | Train Loss1: 0.0865 | Loss2: 0.6134 | Total: 0.2446 || Val Loss1: 0.2874 || Acc1: 0.8939 | AUPRC1: 0.4837\n",
            "Epoch 039 | Train Loss1: 0.0854 | Loss2: 0.6134 | Total: 0.2438 || Val Loss1: 0.2850 || Acc1: 0.8939 | AUPRC1: 0.4808\n",
            "Epoch 040 | Train Loss1: 0.0848 | Loss2: 0.6138 | Total: 0.2435 || Val Loss1: 0.2664 || Acc1: 0.8939 | AUPRC1: 0.3998\n",
            "Epoch 041 | Train Loss1: 0.0849 | Loss2: 0.6133 | Total: 0.2434 || Val Loss1: 0.2688 || Acc1: 0.8939 | AUPRC1: 0.3998\n",
            "Epoch 042 | Train Loss1: 0.0833 | Loss2: 0.6121 | Total: 0.2420 || Val Loss1: 0.2800 || Acc1: 0.9091 | AUPRC1: 0.4474\n",
            "Epoch 043 | Train Loss1: 0.0839 | Loss2: 0.6119 | Total: 0.2423 || Val Loss1: 0.2620 || Acc1: 0.8939 | AUPRC1: 0.4141\n",
            "Epoch 044 | Train Loss1: 0.0830 | Loss2: 0.6124 | Total: 0.2419 || Val Loss1: 0.2612 || Acc1: 0.8939 | AUPRC1: 0.4141\n",
            "Epoch 045 | Train Loss1: 0.0829 | Loss2: 0.6115 | Total: 0.2415 || Val Loss1: 0.2739 || Acc1: 0.8939 | AUPRC1: 0.4617\n",
            "Epoch 046 | Train Loss1: 0.0829 | Loss2: 0.6108 | Total: 0.2412 || Val Loss1: 0.2639 || Acc1: 0.8939 | AUPRC1: 0.4379\n",
            "Epoch 047 | Train Loss1: 0.0819 | Loss2: 0.6114 | Total: 0.2408 || Val Loss1: 0.2585 || Acc1: 0.9091 | AUPRC1: 0.4241\n",
            "Epoch 048 | Train Loss1: 0.0821 | Loss2: 0.6108 | Total: 0.2407 || Val Loss1: 0.2644 || Acc1: 0.8939 | AUPRC1: 0.4663\n",
            "Epoch 049 | Train Loss1: 0.0813 | Loss2: 0.6098 | Total: 0.2399 || Val Loss1: 0.2641 || Acc1: 0.8939 | AUPRC1: 0.4717\n",
            "Epoch 050 | Train Loss1: 0.0812 | Loss2: 0.6100 | Total: 0.2398 || Val Loss1: 0.2545 || Acc1: 0.8939 | AUPRC1: 0.4241\n",
            "Epoch 051 | Train Loss1: 0.0809 | Loss2: 0.6097 | Total: 0.2396 || Val Loss1: 0.2556 || Acc1: 0.8939 | AUPRC1: 0.4782\n",
            "Epoch 052 | Train Loss1: 0.0803 | Loss2: 0.6089 | Total: 0.2389 || Val Loss1: 0.2619 || Acc1: 0.8939 | AUPRC1: 0.5431\n",
            "Epoch 053 | Train Loss1: 0.0802 | Loss2: 0.6088 | Total: 0.2388 || Val Loss1: 0.2529 || Acc1: 0.8939 | AUPRC1: 0.4544\n",
            "Epoch 054 | Train Loss1: 0.0795 | Loss2: 0.6088 | Total: 0.2383 || Val Loss1: 0.2501 || Acc1: 0.8939 | AUPRC1: 0.4544\n",
            "Epoch 055 | Train Loss1: 0.0792 | Loss2: 0.6081 | Total: 0.2379 || Val Loss1: 0.2563 || Acc1: 0.8939 | AUPRC1: 0.4782\n",
            "Epoch 056 | Train Loss1: 0.0789 | Loss2: 0.6077 | Total: 0.2376 || Val Loss1: 0.2512 || Acc1: 0.8939 | AUPRC1: 0.4782\n",
            "Epoch 057 | Train Loss1: 0.0784 | Loss2: 0.6075 | Total: 0.2371 || Val Loss1: 0.2470 || Acc1: 0.8939 | AUPRC1: 0.4278\n",
            "Epoch 058 | Train Loss1: 0.0782 | Loss2: 0.6073 | Total: 0.2369 || Val Loss1: 0.2535 || Acc1: 0.8939 | AUPRC1: 0.5469\n",
            "Epoch 059 | Train Loss1: 0.0777 | Loss2: 0.6071 | Total: 0.2365 || Val Loss1: 0.2516 || Acc1: 0.8939 | AUPRC1: 0.5548\n",
            "Epoch 060 | Train Loss1: 0.0771 | Loss2: 0.6068 | Total: 0.2360 || Val Loss1: 0.2474 || Acc1: 0.8939 | AUPRC1: 0.4623\n",
            "Epoch 061 | Train Loss1: 0.0769 | Loss2: 0.6064 | Total: 0.2357 || Val Loss1: 0.2539 || Acc1: 0.8939 | AUPRC1: 0.5180\n",
            "Epoch 062 | Train Loss1: 0.0764 | Loss2: 0.6060 | Total: 0.2353 || Val Loss1: 0.2526 || Acc1: 0.8939 | AUPRC1: 0.5180\n",
            "Epoch 063 | Train Loss1: 0.0759 | Loss2: 0.6056 | Total: 0.2348 || Val Loss1: 0.2480 || Acc1: 0.8939 | AUPRC1: 0.5337\n",
            "Epoch 064 | Train Loss1: 0.0756 | Loss2: 0.6054 | Total: 0.2345 || Val Loss1: 0.2512 || Acc1: 0.8939 | AUPRC1: 0.5323\n",
            "Epoch 065 | Train Loss1: 0.0751 | Loss2: 0.6053 | Total: 0.2341 || Val Loss1: 0.2493 || Acc1: 0.8939 | AUPRC1: 0.5590\n",
            "Epoch 066 | Train Loss1: 0.0747 | Loss2: 0.6054 | Total: 0.2339 || Val Loss1: 0.2430 || Acc1: 0.8939 | AUPRC1: 0.4782\n",
            "Epoch 067 | Train Loss1: 0.0744 | Loss2: 0.6064 | Total: 0.2340 || Val Loss1: 0.2480 || Acc1: 0.8939 | AUPRC1: 0.6066\n",
            "Epoch 068 | Train Loss1: 0.0739 | Loss2: 0.6081 | Total: 0.2342 || Val Loss1: 0.2476 || Acc1: 0.8939 | AUPRC1: 0.5576\n",
            "Epoch 069 | Train Loss1: 0.0734 | Loss2: 0.6098 | Total: 0.2344 || Val Loss1: 0.2470 || Acc1: 0.8939 | AUPRC1: 0.5337\n",
            "Epoch 070 | Train Loss1: 0.0731 | Loss2: 0.6054 | Total: 0.2328 || Val Loss1: 0.2532 || Acc1: 0.8939 | AUPRC1: 0.5337\n",
            "Epoch 071 | Train Loss1: 0.0727 | Loss2: 0.6038 | Total: 0.2320 || Val Loss1: 0.2505 || Acc1: 0.8939 | AUPRC1: 0.4623\n",
            "Epoch 072 | Train Loss1: 0.0722 | Loss2: 0.6063 | Total: 0.2324 || Val Loss1: 0.2534 || Acc1: 0.8939 | AUPRC1: 0.4722\n",
            "Epoch 073 | Train Loss1: 0.0718 | Loss2: 0.6058 | Total: 0.2320 || Val Loss1: 0.2546 || Acc1: 0.8939 | AUPRC1: 0.4960\n",
            "Epoch 074 | Train Loss1: 0.0714 | Loss2: 0.6033 | Total: 0.2310 || Val Loss1: 0.2524 || Acc1: 0.8939 | AUPRC1: 0.4484\n",
            "Epoch 075 | Train Loss1: 0.0711 | Loss2: 0.6032 | Total: 0.2307 || Val Loss1: 0.2528 || Acc1: 0.8939 | AUPRC1: 0.5175\n",
            "Epoch 076 | Train Loss1: 0.0707 | Loss2: 0.6045 | Total: 0.2309 || Val Loss1: 0.2499 || Acc1: 0.8939 | AUPRC1: 0.5175\n",
            "Epoch 077 | Train Loss1: 0.0704 | Loss2: 0.6037 | Total: 0.2304 || Val Loss1: 0.2466 || Acc1: 0.8939 | AUPRC1: 0.4960\n",
            "Epoch 078 | Train Loss1: 0.0701 | Loss2: 0.6021 | Total: 0.2297 || Val Loss1: 0.2484 || Acc1: 0.8939 | AUPRC1: 0.5175\n",
            "Epoch 079 | Train Loss1: 0.0698 | Loss2: 0.6028 | Total: 0.2297 || Val Loss1: 0.2425 || Acc1: 0.8939 | AUPRC1: 0.5175\n",
            "Epoch 080 | Train Loss1: 0.0694 | Loss2: 0.6034 | Total: 0.2296 || Val Loss1: 0.2417 || Acc1: 0.8939 | AUPRC1: 0.5175\n",
            "Epoch 081 | Train Loss1: 0.0690 | Loss2: 0.6018 | Total: 0.2289 || Val Loss1: 0.2450 || Acc1: 0.8939 | AUPRC1: 0.5175\n",
            "Epoch 082 | Train Loss1: 0.0687 | Loss2: 0.6015 | Total: 0.2285 || Val Loss1: 0.2417 || Acc1: 0.8939 | AUPRC1: 0.5689\n",
            "Epoch 083 | Train Loss1: 0.0684 | Loss2: 0.6023 | Total: 0.2286 || Val Loss1: 0.2424 || Acc1: 0.8939 | AUPRC1: 0.5222\n",
            "Epoch 084 | Train Loss1: 0.0680 | Loss2: 0.6016 | Total: 0.2281 || Val Loss1: 0.2395 || Acc1: 0.8939 | AUPRC1: 0.5222\n",
            "Epoch 085 | Train Loss1: 0.0677 | Loss2: 0.6005 | Total: 0.2276 || Val Loss1: 0.2388 || Acc1: 0.8939 | AUPRC1: 0.5936\n",
            "Epoch 086 | Train Loss1: 0.0673 | Loss2: 0.6005 | Total: 0.2273 || Val Loss1: 0.2384 || Acc1: 0.8939 | AUPRC1: 0.6412\n",
            "Epoch 087 | Train Loss1: 0.0670 | Loss2: 0.6009 | Total: 0.2272 || Val Loss1: 0.2367 || Acc1: 0.8939 | AUPRC1: 0.5936\n",
            "Epoch 088 | Train Loss1: 0.0666 | Loss2: 0.6008 | Total: 0.2269 || Val Loss1: 0.2337 || Acc1: 0.8939 | AUPRC1: 0.5936\n",
            "Epoch 089 | Train Loss1: 0.0663 | Loss2: 0.6000 | Total: 0.2264 || Val Loss1: 0.2324 || Acc1: 0.8939 | AUPRC1: 0.5936\n",
            "Epoch 090 | Train Loss1: 0.0658 | Loss2: 0.5993 | Total: 0.2259 || Val Loss1: 0.2331 || Acc1: 0.9091 | AUPRC1: 0.6412\n",
            "Epoch 091 | Train Loss1: 0.0656 | Loss2: 0.5992 | Total: 0.2256 || Val Loss1: 0.2301 || Acc1: 0.9091 | AUPRC1: 0.6198\n",
            "Epoch 092 | Train Loss1: 0.0653 | Loss2: 0.5994 | Total: 0.2255 || Val Loss1: 0.2363 || Acc1: 0.9242 | AUPRC1: 0.5936\n",
            "Epoch 093 | Train Loss1: 0.0650 | Loss2: 0.5995 | Total: 0.2254 || Val Loss1: 0.2300 || Acc1: 0.9091 | AUPRC1: 0.5619\n",
            "Epoch 094 | Train Loss1: 0.0648 | Loss2: 0.5991 | Total: 0.2251 || Val Loss1: 0.2394 || Acc1: 0.9242 | AUPRC1: 0.6398\n",
            "Epoch 095 | Train Loss1: 0.0645 | Loss2: 0.5985 | Total: 0.2247 || Val Loss1: 0.2290 || Acc1: 0.9091 | AUPRC1: 0.6477\n",
            "Epoch 096 | Train Loss1: 0.0645 | Loss2: 0.5979 | Total: 0.2245 || Val Loss1: 0.2405 || Acc1: 0.9242 | AUPRC1: 0.6398\n"
          ]
        }
      ]
    }
  ]
}