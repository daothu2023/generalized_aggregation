{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daothu2023/generalized_aggregation/blob/main/Thu_Prediction_Gene__PPI_String_gui_Dinh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgwuCX9IngXk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BED6Lk9Qq-Q",
        "outputId": "fe691a4e-db68-4311-d2f9-cfef3ab19564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOHBR5RwlBRh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# ---------- 1. ƒê·ªçc d·ªØ li·ªáu ----------\n",
        "edges_df = pd.read_csv('/content/drive/My Drive/PPI_STRING/ppi_for_gnn_filled.csv')\n",
        "features_df = pd.read_csv('/content/drive/My Drive/PPI_STRING/features_for_BRCA.csv', index_col=0)\n",
        "labels_df = pd.read_csv('/content/drive/My Drive/PPI_STRING/BRCA_labels(0_1).csv')\n",
        "\n",
        "# ---------- 2. Danh s√°ch t·∫•t c·∫£ gene t·ª´ PPI ----------\n",
        "genes_from_edges = set(edges_df['protein1']).union(set(edges_df['protein2']))\n",
        "genes_from_features = set(features_df.index)\n",
        "all_genes = sorted(genes_from_edges)  # ƒë·∫£m b·∫£o th·ª© t·ª± c·ªë ƒë·ªãnh\n",
        "\n",
        "# Mapping gene <-> index\n",
        "node_to_idx = {gene: i for i, gene in enumerate(all_genes)}\n",
        "idx_to_node = {i: gene for gene, i in node_to_idx.items()}\n",
        "\n",
        "# ---------- 3. edge_index ----------\n",
        "edges = edges_df[['protein1', 'protein2']].dropna()\n",
        "edge_index = torch.tensor([[node_to_idx[a], node_to_idx[b]]\n",
        "                           for a, b in edges.values if a in node_to_idx and b in node_to_idx],\n",
        "                          dtype=torch.long).t().contiguous()\n",
        "\n",
        "# ---------- 4. T·∫°o ƒë·∫∑c tr∆∞ng x ----------\n",
        "feature_dim = features_df.shape[1]\n",
        "x_matrix = np.zeros((len(all_genes), feature_dim))\n",
        "has_feature = np.zeros(len(all_genes), dtype=bool)\n",
        "\n",
        "# Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features_df.values)\n",
        "features_scaled_df = pd.DataFrame(features_scaled, index=features_df.index)\n",
        "\n",
        "# G√°n features cho nh·ªØng gene c√≥ s·∫µn\n",
        "for gene in features_scaled_df.index:\n",
        "    if gene in node_to_idx:\n",
        "        idx = node_to_idx[gene]\n",
        "        x_matrix[idx] = features_scaled_df.loc[gene].values\n",
        "        has_feature[idx] = True\n",
        "\n",
        "# T√≠nh h√†ng x√≥m\n",
        "neighbors_dict = {i: [] for i in range(len(all_genes))}\n",
        "for src, dst in edge_index.t().tolist():\n",
        "    neighbors_dict[src].append(dst)\n",
        "    neighbors_dict[dst].append(src)\n",
        "\n",
        "# G√°n ƒë·∫∑c tr∆∞ng trung b√¨nh h√†ng x√≥m cho node thi·∫øu\n",
        "for i in range(len(all_genes)):\n",
        "    if not has_feature[i]:\n",
        "        neighbor_feats = [x_matrix[n] for n in neighbors_dict[i] if has_feature[n]]\n",
        "        if neighbor_feats:\n",
        "            x_matrix[i] = np.mean(neighbor_feats, axis=0)\n",
        "        # n·∫øu kh√¥ng c√≥ h√†ng x√≥m n√†o c√≥ feature th√¨ gi·ªØ nguy√™n (to√†n 0)\n",
        "\n",
        "x = torch.tensor(x_matrix, dtype=torch.float)\n",
        "\n",
        "# ---------- 5. T·∫°o nh√£n ----------\n",
        "labels_map = {row['Gene']: row['Labels'] for _, row in labels_df.iterrows()}\n",
        "y = torch.full((x.size(0),), -1, dtype=torch.long)  # g√°n m·∫∑c ƒë·ªãnh -1\n",
        "\n",
        "for gene, label in labels_map.items():\n",
        "    if gene in node_to_idx:\n",
        "        y[node_to_idx[gene]] = int(label)\n",
        "\n",
        "# ---------- 6. T·∫°o Data ----------\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "# data.train_mask = y != -1  # mask c√°c node c√≥ nh√£n\n",
        "\n",
        "# print(data)\n",
        "# print(\"S·ªë ƒë·ªânh c√≥ nh√£n:\", data.train_mask.sum().item())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import average_precision_score\n",
        "import numpy as np\n",
        "\n",
        "# GCN Model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.fc1 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
        "        self.fc2 = torch.nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Evaluate\n",
        "def evaluate(model, data, mask, loss_fn):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_mask = mask & (data.y != -1)\n",
        "        out = model(data.x, data.edge_index)\n",
        "        probs = F.softmax(out, dim=1)\n",
        "        loss = loss_fn(out[valid_mask], data.y[valid_mask]).item()\n",
        "        preds = out[valid_mask].argmax(dim=1)\n",
        "        acc = (preds == data.y[valid_mask]).sum().item() / valid_mask.sum().item()\n",
        "        auprc = average_precision_score(data.y[valid_mask].cpu(), probs[valid_mask][:, 1].cpu())\n",
        "    return acc, auprc, loss\n",
        "\n",
        "# Train\n",
        "def train_one_epoch(model, data, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    mask = data.train_mask & (data.y != -1)\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = loss_fn(out[mask], data.y[mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Oversampling helper\n",
        "def oversample_features(x, y, idx):\n",
        "    \"\"\"L·∫∑p l·∫°i d·ªØ li·ªáu l·ªõp thi·ªÉu s·ªë ƒë·ªÉ c√¢n b·∫±ng\"\"\"\n",
        "    from collections import Counter\n",
        "    labels = y[idx].cpu().numpy()\n",
        "    class_counts = Counter(labels)\n",
        "    max_class = max(class_counts.values())\n",
        "\n",
        "    new_x = []\n",
        "    new_y = []\n",
        "    for c in class_counts:\n",
        "        c_idx = idx[(y[idx] == c)]\n",
        "        repeats = max_class - class_counts[c]\n",
        "        repeat_idx = c_idx.repeat(int(repeats // len(c_idx)) + 1)[:repeats]\n",
        "        new_x.append(x[repeat_idx])\n",
        "        new_y.append(y[repeat_idx])\n",
        "    if new_x:\n",
        "        x_added = torch.cat(new_x)\n",
        "        y_added = torch.cat(new_y)\n",
        "        return torch.cat([x, x_added]), torch.cat([y, y_added])\n",
        "    else:\n",
        "        return x, y\n",
        "lr=0.01\n",
        "weight_decay=5e-4\n",
        "hidden_channels = 32\n",
        "epoch = 200\n",
        "\n",
        "# Run training\n",
        "def run_gcn_with_oversampling(data, hidden_channels):\n",
        "    labeled_idx = torch.where(data.y != -1)[0]\n",
        "    labeled_y = data.y[labeled_idx]\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    results = []\n",
        "\n",
        "    for fold, (train_val_idx, test_idx) in enumerate(skf.split(labeled_idx, labeled_y)):\n",
        "        print(f\"\\nüìÇ Fold {fold+1}/5\")\n",
        "\n",
        "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "        train_idx, val_idx = next(sss.split(train_val_idx, labeled_y[train_val_idx]))\n",
        "\n",
        "        train_nodes = labeled_idx[train_val_idx][train_idx]\n",
        "        val_nodes = labeled_idx[train_val_idx][val_idx]\n",
        "        test_nodes = labeled_idx[test_idx]\n",
        "\n",
        "        # Oversampling tr√™n train_nodes\n",
        "        x_train = data.x[train_nodes]\n",
        "        y_train = data.y[train_nodes]\n",
        "        x_resampled, y_resampled = oversample_features(x_train, y_train, torch.arange(len(train_nodes)))\n",
        "\n",
        "        data.x = torch.cat([data.x, x_resampled[len(train_nodes):]], dim=0)\n",
        "        data.y = torch.cat([data.y, y_resampled[len(train_nodes):]], dim=0)\n",
        "\n",
        "        n_total = data.x.shape[0]\n",
        "        train_mask = torch.zeros(n_total, dtype=torch.bool)\n",
        "        val_mask = torch.zeros(n_total, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(n_total, dtype=torch.bool)\n",
        "\n",
        "        train_mask[train_nodes] = True\n",
        "        train_mask[len(train_nodes):] = True  # Mask cho c√°c node oversampled\n",
        "        val_mask[val_nodes] = True\n",
        "        test_mask[test_nodes] = True\n",
        "\n",
        "        data.train_mask = train_mask\n",
        "        data.val_mask = val_mask\n",
        "        data.test_mask = test_mask\n",
        "\n",
        "        # Weighted loss\n",
        "        y_train_all = torch.cat([y_train, y_resampled[len(train_nodes):]]).cpu().numpy()\n",
        "        weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_all), y=y_train_all)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float32))\n",
        "\n",
        "        model = GCN(data.num_node_features, hidden_channels, out_channels=2)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        for epoch in range(1, epoch):\n",
        "            train_loss = train_one_epoch(model, data, optimizer, loss_fn)\n",
        "            train_acc, train_auprc, _ = evaluate(model, data, train_mask, loss_fn)\n",
        "            val_acc, val_auprc, val_loss = evaluate(model, data, val_mask, loss_fn)\n",
        "\n",
        "            print(f\"Epoch {epoch:03d} | Train Acc: {train_acc:.4f} | AUPRC: {train_auprc:.4f} || \"\n",
        "                  f\"Val Acc: {val_acc:.4f} | AUPRC: {val_auprc:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = model.state_dict()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            if patience_counter >= 40:\n",
        "                break\n",
        "\n",
        "        model.load_state_dict(best_model)\n",
        "        test_acc, test_auprc, _ = evaluate(model, data, test_mask, loss_fn)\n",
        "        print(f\"‚úÖ Test Accuracy: {test_acc:.4f} | AUPRC: {test_auprc:.4f}\")\n",
        "        results.append((test_acc, test_auprc))\n",
        "\n",
        "    accs, auprcs = zip(*results)\n",
        "    print(\"\\nüìä T·ªïng k·∫øt sau 5 fold:\")\n",
        "    print(f\"Accuracy: {np.mean(accs):.4f} ¬± {np.std(accs):.4f}\")\n",
        "    print(f\"AUPRC:    {np.mean(auprcs):.4f} ¬± {np.std(auprcs):.4f}\")\n",
        "\n",
        "# G·ªçi ch·∫°y:\n",
        "run_gcn_with_oversampling(data)\n"
      ],
      "metadata": {
        "id": "wDGblPUiXWOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9nPmckyKqUM"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhp8ltnmCkCOr0FnOZY9ZS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}